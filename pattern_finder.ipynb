{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vosA89WEixQc"
   },
   "source": [
    "# pandas temporal query language\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9smRfNEU7kL"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import singledispatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJgdBxlWEy-r"
   },
   "source": [
    "# General helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qG9pbooAEst9"
   },
   "source": [
    "## Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVQwSQ67O_8x"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class Info():\n",
    "  \"\"\"\n",
    "  A class to store information about the data and results from analysis\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "      self.evaluated = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4-2Ic_OF3Gl"
   },
   "source": [
    "## memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rpi14aJEFxot"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def memory(info, func, expr):\n",
    "  \"\"\"\n",
    "  checks if the function has been called with the same argument previously and\n",
    "  if so, returns the same results instead of running the function again\n",
    "\n",
    "  args:\n",
    "    -\n",
    "  \"\"\"\n",
    "  rows=None\n",
    "  if info:\n",
    "    if func in info.evaluated:\n",
    "      if expr in info.evaluated[func]:\n",
    "        rows = info.evaluated[func][expr]\n",
    "    else:\n",
    "      info.evaluated[func] = {}\n",
    "  else:\n",
    "    info = Info()\n",
    "    info.evaluated[func] = {}\n",
    "  return info, rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9tHjHXQEoAo"
   },
   "source": [
    "## listify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBF5ZPs2EoAw"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def listify(string_or_list):\n",
    "    \"\"\"\n",
    "    return a list if the input is a string, if not: returns the input as it was\n",
    "\n",
    "    Args:\n",
    "        string_or_list (str or any):\n",
    "\n",
    "    Returns:\n",
    "        A list if the input is a string, if not: returns the input as it was\n",
    "\n",
    "    Note:\n",
    "        - allows user to use a string as an argument instead of single lists\n",
    "        - cols='icd10' is allowed instead of cols=['icd10']\n",
    "        - cols='icd10' is transformed to cols=['icd10'] by this function\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(string_or_list, str):\n",
    "        string_or_list = [string_or_list]\n",
    "    return string_or_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YQhtgvn7EUSN"
   },
   "source": [
    "## unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCH02_MBEY_s"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# A function to identify all unique values in one or more columns\n",
    "# with one or multiple codes in each cell\n",
    "\n",
    "def unique(df, cols=None, sep=None, all_str=True):\n",
    "  \"\"\"\n",
    "  Lists unique values from one or more columns\n",
    "\n",
    "  sep (str): separator if cells have multiple values\n",
    "  all_str (bool): converts all values to strings\n",
    "\n",
    "  unique(df=df, cols='inpatient', sep=',')\n",
    "  \"\"\"\n",
    "  # if no column(s) are specified, find unique values in whole dataframe\n",
    "  if cols==None:\n",
    "    cols=list(df.columns)\n",
    "  cols = listify(cols)\n",
    "\n",
    "  # multiple values with separator in cells\n",
    "  if sep:\n",
    "    all_unique=set()\n",
    "\n",
    "    for col in cols:\n",
    "      new_unique = set(df[col].str.cat(sep=',').split(','))\n",
    "      all_unique.update(new_unique)\n",
    "  # single valued cells\n",
    "  else:\n",
    "    all_unique = pd.unique(df[cols].values.ravel('K'))\n",
    "\n",
    "  # if need to make sure all elements are strings without surrounding spaces\n",
    "  if all_str:\n",
    "    all_unique=[str(value).strip() for value in all_unique]\n",
    "\n",
    "  return all_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z69nIn-KK4az"
   },
   "outputs": [],
   "source": [
    "#unique(df=df, cols='codes', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "goKBivTUFNoU"
   },
   "source": [
    "## del dot and zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nk49gESsFNoY"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def del_dot(code):\n",
    "  if isinstance(code, str):\n",
    "    return code.replace('.','')\n",
    "  else:\n",
    "    codes = [c.replace('.','') for c in code]\n",
    "  return codes\n",
    "\n",
    "def del_zero(code, left=True, right=False):\n",
    "  if isinstance(codes, str):\n",
    "    codes=[code]\n",
    "  if left:\n",
    "    codes = [c.lstrip('0') for c in code]\n",
    "  if right:\n",
    "    codes = [c.rstrip('0') for c in code]\n",
    "  if isinstance(code, str):\n",
    "    codes=codes[0]\n",
    "  return codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hxm-5QiEFe60"
   },
   "source": [
    "# Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bEOTYsewSkaS"
   },
   "source": [
    "## expand hyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OwcbGI7Br2X"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# function to expand a string like 'K51.2-K53.8' to a list of codes\n",
    "\n",
    "# Need regex to extract the number component of the input string\n",
    "\n",
    "# The singledispach decorator enables us to have the same name, but use\n",
    "# different functions depending on the datatype of the first argument.\n",
    "#\n",
    "# In our case we want one function to deal with a single string input, and\n",
    "# another to handle a list of strings. It could all be handled in a single\n",
    "# function using nested if, but singledispatch makes it less messy and more fun!\n",
    "\n",
    "\n",
    "# Here is the main function, it is just the name and an error message if the\n",
    "# argument does not fit any of the inputs that wil be allowed\n",
    "\n",
    "@singledispatch\n",
    "def expand_hyphen(expr):\n",
    "  \"\"\"\n",
    "  Expands codes expression(s) that have hyphens to list of all codes\n",
    "\n",
    "  Args:\n",
    "      code (str or list of str): String or list of strings to be expanded\n",
    "\n",
    "  Returns:\n",
    "      List of strings\n",
    "\n",
    "  Examples:\n",
    "      expand_hyphen('C00*-C26*')\n",
    "      expand_hyphen('b01.1*-b09.9*')\n",
    "      expand_hyphen('n02.2-n02.7')\n",
    "      expand_hyphen('c00*-c260')\n",
    "      expand_hyphen('b01-b09')\n",
    "      expand_hyphen('b001.1*-b009.9*')\n",
    "      expand_hyphen(['b001.1*-b009.9*', 'c11-c15'])\n",
    "  Note:\n",
    "      Unequal number of decimals in start and end code is problematic.\n",
    "      Example: C26.0-C27.11 will not work since the meaning is not obvious:\n",
    "      Is the step size 0.01? In which case C27.1 will not be included, while\n",
    "      C27.10 will be (and traing zeros can be important in codes)\n",
    "  \"\"\"\n",
    "  raise ValueError('The argument must be a string or a list')\n",
    "\n",
    "# register the function to be used if the input is a string\n",
    "@expand_hyphen.register(str)\n",
    "def _(expr):\n",
    "    # return immediately if nothing to expand\n",
    "    if '-' not in expr:\n",
    "      return [expr]\n",
    "\n",
    "    lower, upper = expr.split('-')\n",
    "\n",
    "    lower=lower.strip()\n",
    "\n",
    "    # identify the numeric component of the code\n",
    "    lower_str = re.search(\"\\d*\\.\\d+|\\d+\", lower).group()\n",
    "    upper_str = re.search(\"\\d*\\.\\d+|\\d+\", upper).group()\n",
    "    # note: what about european decimal notation?\n",
    "    # also note: what if multiple groups K50.1J8.4-etc\n",
    "\n",
    "\n",
    "    lower_num = int(lower_str.replace('.',''))\n",
    "    upper_num = int(upper_str.replace('.','')) +1\n",
    "\n",
    "    if upper_num<lower_num:\n",
    "      raise ValueError('The start code cannot have a higher number than the end code')\n",
    "\n",
    "    # remember length in case of leading zeros\n",
    "    length = len(lower_str)\n",
    "\n",
    "    nums = range(lower_num, upper_num)\n",
    "\n",
    "    # must use integers in a loop, not floats\n",
    "    # which also means that we must multiply and divide to get decimal back\n",
    "    # and take care of leading and trailing zeros that may disappear\n",
    "    if '.' in lower_str:\n",
    "      lower_decimals = len(lower_str.split('.')[1])\n",
    "      upper_decimals = len(upper_str.split('.')[1])\n",
    "      if lower_decimals==upper_decimals:\n",
    "        multiplier = 10**lower_decimals\n",
    "        codes = [lower.replace(lower_str, format(num /multiplier, f'.{lower_decimals}f').zfill(length)) for num in nums]\n",
    "      # special case: allow k1.1-k1.123, but not k.1-k2.123 the last is ambigious: should it list k2.0 only 2.00?\n",
    "      elif (lower_decimals<upper_decimals) & (upper_str.split('.')[0]==lower_str.split('.')[0]):\n",
    "        from_decimal = int(lower_str.split('.')[1])\n",
    "        to_decimal = int(upper_str.split('.')[1]) +1\n",
    "        nums = range(from_decimal, to_decimal)\n",
    "        decimal_str = '.'+lower.split('.')[1]\n",
    "        codes = [lower.replace(decimal_str, '.'+str(num)) for num in nums]\n",
    "      else:\n",
    "        raise ValueError('The start code and the end code do not have the same number of decimals')\n",
    "    else:\n",
    "        codes = [lower.replace(lower_str, str(num).zfill(length)) for num in nums]\n",
    "    return codes\n",
    "\n",
    "\n",
    "# register the function to be used if if the input is a list of strings\n",
    "@expand_hyphen.register(list)\n",
    "def _(expr):\n",
    "  extended = []\n",
    "  for word in expr:\n",
    "    extended.extend(expand_hyphen(word))\n",
    "  return extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OU1AIHQiRJRn"
   },
   "source": [
    "## expand star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rb0AiMPhd0Nj"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# A function to expand a string with star notation (K50*)\n",
    "# to list of all codes starting with K50\n",
    "\n",
    "@singledispatch\n",
    "def expand_star(code, all_codes=None):\n",
    "  \"\"\"\n",
    "  Expand expressions with star notation to a list of all values with the specified pattern\n",
    "\n",
    "  Args:\n",
    "    expr (str or list): Expression (or list of expressions) to be expanded\n",
    "    all_codes (list) : A list of all codes\n",
    "\n",
    "  Examples:\n",
    "    expand_star('K50*', all_codes=icd9)\n",
    "    expand_star('K*5', all_codes=icd9)\n",
    "    expand_star('*5', all_codes=icd9)\n",
    "\n",
    "  \"\"\"\n",
    "  raise ValueError('The argument must be a string or a list')\n",
    "\n",
    "@expand_star.register(str)\n",
    "def _(code, all_codes=None):\n",
    "  # return immediately if there is nothing to expand\n",
    "  if '*' not in code:\n",
    "    return [code]\n",
    "\n",
    "  start_str, end_str = code.split('*')\n",
    "\n",
    "  if start_str and end_str:\n",
    "    codes = {code for code in all_codes if (code.startswith(start_str) & code.endswith(end_str))}\n",
    "\n",
    "  if start_str:\n",
    "    codes = {code for code in all_codes if code.startswith(start_str)}\n",
    "\n",
    "  if end_str:\n",
    "    codes = {code for code in all_codes if code.endswith(end_str)}\n",
    "\n",
    "  return sorted(list(codes))\n",
    "\n",
    "@expand_star.register(list)\n",
    "def _(code, all_codes=None):\n",
    "\n",
    "  expanded=[]\n",
    "  for star_code in code:\n",
    "    new_codes = expand_star(star_code, all_codes=all_codes)\n",
    "    expanded.extend(new_codes)\n",
    "\n",
    "  # uniqify in case some overlap\n",
    "  expanded = list(set(expanded))\n",
    "\n",
    "  return sorted(expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4T6rBQZsiwzT"
   },
   "source": [
    "## expand colon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTHKj3y3__kn"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# function to get all codes in a list between the specified start and end code\n",
    "# Example: Get all codes between K40:L52\n",
    "\n",
    "@singledispatch\n",
    "def expand_colon(code, all_codes=None):\n",
    "  raise ValueError('The argument must be a string or a list')\n",
    "\n",
    "@expand_colon.register(str)\n",
    "def _(code, all_codes=None):\n",
    "  \"\"\"\n",
    "  Expand expressions with colon notation to a list of complete code names\n",
    "  code (str or list): Expression (or list of expressions) to be expanded\n",
    "  all_codes (list or array) : The list to slice from\n",
    "\n",
    "  Examples\n",
    "    K50:K52\n",
    "    K50.5:K52.19\n",
    "    A3.0:A9.3\n",
    "\n",
    "  Note: This is different from hyphen and star notation because it can handle\n",
    "  different code lengths and different number of decimals\n",
    "\n",
    "  \"\"\"\n",
    "  if ':' not in code:\n",
    "    return [code]\n",
    "\n",
    "  startstr, endstr = code.split(':')\n",
    "\n",
    "  # remove spaces\n",
    "  startstr = startstr.strip()\n",
    "  endstr =endstr.strip()\n",
    "\n",
    "  # find start and end position\n",
    "  startpos = all_codes.index(startstr)\n",
    "  endpos = all_codes.index(endstr) + 1\n",
    "\n",
    "  # slice list\n",
    "  expanded = all_codes[startpos:endpos+1]\n",
    "\n",
    "  return expanded\n",
    "\n",
    "\n",
    "@expand_colon.register(list)\n",
    "def _(code, all_codes=None, regex=False):\n",
    "  expanded=[]\n",
    "\n",
    "  for cod in code:\n",
    "    new_codes = expand_colon(cod, all_codes=all_codes)\n",
    "    expanded.extend(new_codes)\n",
    "\n",
    "  return expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_WwFa6iai3Wq"
   },
   "source": [
    "## expand regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NbPMX0yS4Hmj"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# Return all elements in a list that fits a regex pattern\n",
    "\n",
    "@singledispatch\n",
    "def expand_regex(code, all_codes):\n",
    "  raise ValueError('The argument must be a string or a list of strings')\n",
    "\n",
    "@expand_regex.register(str)\n",
    "def _(code, all_codes=None):\n",
    "  code_regex = re.compile(code)\n",
    "  expanded = {code for code in all_codes if code_regex.match(code)}\n",
    "  # uniqify\n",
    "  expanded = list(set(expanded))\n",
    "  return expanded\n",
    "\n",
    "@expand_regex.register(list)\n",
    "def _(code, all_codes):\n",
    "  expanded=[]\n",
    "\n",
    "  for cod in code:\n",
    "    new_codes = expand_regex(cod, all_codes=all_codes)\n",
    "    expanded.extend(new_codes)\n",
    "\n",
    "  # uniqify in case some overlap\n",
    "  expanded = sorted(list(set(expanded)))\n",
    "\n",
    "  return expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iNZymQRsi8oW"
   },
   "source": [
    "## expand code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpZQsDVgu1hr"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "@singledispatch\n",
    "def expand_code(code, all_codes=None,\n",
    "                hyphen=True, star=True, colon=True, regex=False,\n",
    "                drop_dot=False, drop_leading_zero=False,\n",
    "                sort_unique=True):\n",
    "  raise ValueError('The argument must be a string or a list of strings')\n",
    "\n",
    "@expand_code.register(str)\n",
    "def _(code, all_codes=None,\n",
    "      hyphen=True, star=True, colon=True, regex=False,\n",
    "      drop_dot=False, drop_leading_zero=False,\n",
    "      sort_unique=True):\n",
    "  #validating input\n",
    "  if (not regex) and (':' in code) and (('-' in code) or ('*' in code)):\n",
    "    raise ValueError('Notation using colon must start from and end in specific codes, not codes using star or hyphen')\n",
    "\n",
    "  if regex:\n",
    "    codes = expand_regex(code, all_codes=all_codes)\n",
    "    return codes\n",
    "\n",
    "  if drop_dot:\n",
    "    code = del_dot(code)\n",
    "\n",
    "  codes=[code]\n",
    "\n",
    "  if hyphen:\n",
    "    codes=expand_hyphen(code)\n",
    "  if star:\n",
    "    codes=expand_star(codes, all_codes=all_codes)\n",
    "  if colon:\n",
    "    codes=expand_colon(codes, all_codes=all_codes)\n",
    "\n",
    "  if sort_unique:\n",
    "    codes = sorted(list(set(codes)))\n",
    "\n",
    "  return codes\n",
    "\n",
    "@expand_code.register(list)\n",
    "def _(code, all_codes=None, hyphen=True, star=True, colon=True, regex=False,\n",
    "      drop_dot=False, drop_leading_zero=False,\n",
    "      sort_unique=True):\n",
    "\n",
    "  expanded=[]\n",
    "\n",
    "  for cod in code:\n",
    "    new_codes = expand_code(cod, all_codes=all_codes, hyphen=hyphen, star=star, colon=colon, regex=regex, drop_dot=drop_dot, drop_leading_zero=drop_leading_zero)\n",
    "    expanded.extend(new_codes)\n",
    "\n",
    "  # uniqify in case some overlap\n",
    "  expanded = list(set(expanded))\n",
    "\n",
    "  return sorted(expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4E02nphLRAH"
   },
   "source": [
    "## expand columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PaSOcMjQLYHf"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def expand_columns(expr, all_columns=None, df=None, star=True,\n",
    "                   hyphen=True, colon=True, regex=None, info=None):\n",
    "    \"\"\"\n",
    "    Expand columns with special notation to their full column names\n",
    "\n",
    "    \"\"\"\n",
    "    notations = '* - :'.split()\n",
    "    # return immediately if not needed\n",
    "    if not any(symbol in expr for symbol in notations):\n",
    "      return [expr]\n",
    "\n",
    "    # get a list of columns of it is only implicity defined by the df\n",
    "    # warning: may depreciate this, require explicit all_columns\n",
    "    if df & (not all_columns):\n",
    "      all_columns=list(df.columns)\n",
    "\n",
    "    if regex:\n",
    "      cols = [col for col in all_columns if re.match(regex, expr)]\n",
    "    else:\n",
    "      if hyphen:\n",
    "        cols = expand_hyphen(expr)\n",
    "      if star:\n",
    "        cols = expand_star(expr, all_codes=all_columns)\n",
    "      if colon:\n",
    "        cols = expand_colon(expr, all_codes=all_columns)\n",
    "\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBlNB_bjFa0I"
   },
   "source": [
    "# More helper functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cki3g2TkTPPY"
   },
   "source": [
    "## get rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqIDq3lEiGsL"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# mark rows that contain certain codes in one or more colums\n",
    "def get_rows(df, codes, cols=None, sep=None, pid='pid', info=None, fix=True):\n",
    "  \"\"\"\n",
    "  Make a boolean series that is true for all rows that contain the codes\n",
    "\n",
    "  Args\n",
    "    df (dataframe or series): The dataframe with codes\n",
    "    codes (str, list, set, dict): codes to be counted\n",
    "    cols (str or list): list of columns to search in\n",
    "    sep (str): The symbol that seperates the codes if there are multiple codes in a cell\n",
    "    pid (str): The name of the column with the personal identifier\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  # check if evaluated previously\n",
    "  info, rows = memory(info=info, func = 'get_rows', expr=codes)\n",
    "  if rows:\n",
    "    return rows\n",
    "\n",
    "  # check if codes and columns need to be expanded (needed if they use notation)\n",
    "  if fix:\n",
    "    # do this when if cols exist, but if it does not ...\n",
    "    cols = expand_columns(expr=cols, all_columns=list(df.columns), info=info)\n",
    "    all_codes = sorted(unique(df=df, cols=cols, sep=sep))\n",
    "    codes = expand_code(codes, all_codes=all_codes)\n",
    "\n",
    "  # codes and cols should be lists\n",
    "  codes = listify(codes)\n",
    "  cols = listify(cols)\n",
    "\n",
    "  # approach depends on whether we have multi-value cells or not\n",
    "  # if sep exist, then have multi-value cells\n",
    "  if sep:\n",
    "    # have multi-valued cells\n",
    "    # note: this assumes the sep is a regex word delimiter\n",
    "    codes = [rf'\\b{code}\\b' for code in codes]\n",
    "    codes_regex = '|'.join(codes)\n",
    "\n",
    "    # starting point: no codes have been found\n",
    "    # needed since otherwise the function might return None if no codes exist\n",
    "    rows = pd.Series(False*len(df),index=df.index)\n",
    "\n",
    "   # loop over all columns and mark when a code exist\n",
    "    for col in cols:\n",
    "      rows=rows | df[col].str.contains(codes_regex, na=False)\n",
    "\n",
    "  # if not multi valued cells\n",
    "  else:\n",
    "    mask = df[cols].isin(codes)\n",
    "    rows = mask.any(axis=1)\n",
    "  return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBFksM_qU2Lv"
   },
   "source": [
    "## make codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96CSiOu7ied9"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def make_codes(n, letters=26, numbers=100, seed=False):\n",
    "  \"\"\"\n",
    "  Generate a dataframe with a column of random codes\n",
    "\n",
    "  Args:\n",
    "    letters (int): The number of different letters to use\n",
    "    numbers (int): The number of different numbers to use\n",
    "\n",
    "  Returns\n",
    "    A dataframe with a column with one or more codes in the rows\n",
    "\n",
    "  \"\"\"\n",
    "  # each code is assumed to consist of a letter and a number\n",
    "  alphabet = list('abcdefghigjklmnopqrstuvwxyz')\n",
    "  letters=alphabet[:letters+1]\n",
    "\n",
    "  # make random numbers same if seed is specified\n",
    "  if seed:\n",
    "    np.random.seed(0)\n",
    "\n",
    "  # determine the number of codes to be drawn for each event\n",
    "  n_codes=np.random.negative_binomial(1, p=0.3, size=n)\n",
    "  # avoid zero (all events have to have at least one code)\n",
    "  n_codes=n_codes+1\n",
    "\n",
    "  # for each event, randomly generate a the number of codes specified by n_codes\n",
    "  codes=[]\n",
    "  for i in n_codes:\n",
    "      diag = [np.random.choice(letters).upper()+\n",
    "              str(int(np.random.uniform(low=1, high=numbers)))\n",
    "              for num in range(i)]\n",
    "\n",
    "      code_string=','.join(diag)\n",
    "      codes.append(code_string)\n",
    "\n",
    "  # create a dataframe based on the list\n",
    "  df=pd.DataFrame(codes)\n",
    "  df.columns=['code']\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f6Aej2nRyVX4"
   },
   "outputs": [],
   "source": [
    "make_codes(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gsxSAgv_VKr4"
   },
   "source": [
    "## make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3H5egzAjapK"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def make_data(n, letters=26, numbers=100, seed=False):\n",
    "  \"\"\"\n",
    "  Generate a dataframe with a column of random codes\n",
    "\n",
    "  Args:\n",
    "    letters (int): The number of different letters to use\n",
    "    numbers (int): The number of different numbers to use\n",
    "\n",
    "  Returns\n",
    "    A dataframe with a column with one or more codes in the rows\n",
    "\n",
    "  \"\"\"\n",
    "  pid = range(n)\n",
    "  df_person=pd.DataFrame(index = pid)\n",
    "\n",
    "  #female = np.random.binomial(1, 0.5, size =n)\n",
    "  gender = np.random.choice(['male', 'female'], size=n)\n",
    "  region = np.random.choice(['north', 'south', 'east', 'west'], size=n)\n",
    "  birth_year = np.random.randint(1920, 1980, size=n)\n",
    "  birth_month = np.random.randint(1,12, size=n)\n",
    "  birth_day = np.random.randint(1,28, size=n) # ok, I know!\n",
    "  events_per_year = np.random.poisson(1, size=n)\n",
    "  years = 2020 - birth_year\n",
    "  events = years * events_per_year\n",
    "  events = np.where(events==0,1,events)\n",
    "  events = events.astype(int)\n",
    "  all_codes=[]\n",
    "  codes = [all_codes.extend(make_codes(n=n, letters=letters,\n",
    "                                       numbers=numbers,\n",
    "                                       seed=seed)['code'].tolist())\n",
    "          for n in events]\n",
    "\n",
    "  days_alive = (2020 - birth_year) *365\n",
    "\n",
    "  days_and_events = zip(days_alive.tolist(), events.tolist())\n",
    "  all_days=[]\n",
    "  days_after_birth = [all_days.extend(np.random.randint(0, max_day, size=n)) for max_day, n in days_and_events]\n",
    "  pid_and_events = zip(list(pid), events.tolist())\n",
    "  all_pids=[]\n",
    "  pids = [all_pids.extend([p+1]*e) for p, e in pid_and_events]\n",
    "\n",
    "  df_events = pd.DataFrame(index=all_pids)\n",
    "  df_events['codes'] = all_codes\n",
    "  df_events['days_after'] = all_days\n",
    "\n",
    "  #df_person['female'] = female\n",
    "  df_person['gender'] = gender\n",
    "\n",
    "  df_person['region'] = region\n",
    "  df_person['year'] = birth_year\n",
    "  df_person['month'] = birth_month\n",
    "  df_person['day'] = birth_day\n",
    "  df = df_events.merge(df_person, left_index=True, right_index=True)\n",
    "  df['birth_date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "  df['event_date'] = df['birth_date'] + pd.to_timedelta(df.days_after, unit='d')\n",
    "  del df['month']\n",
    "  del df['day']\n",
    "  del df['days_after']\n",
    "  df['pid'] = df.index\n",
    "  df.index_name = 'pid_index'\n",
    "  df=df[['pid', 'gender', 'birth_date', 'event_date', 'region', 'codes']]\n",
    "  # include deaths too?\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEo-95j2ydvm"
   },
   "outputs": [],
   "source": [
    "#df = make_data(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AX_5I3DKwZhO"
   },
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vd6PxB8Cwx2p"
   },
   "outputs": [],
   "source": [
    "#count_person('max 2 L35')\n",
    "#count_person('x before y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_codes(df, codes, cols=None, sep=None, new_sep=',', na_rep='',\n",
    "                  prefix=None, merge=False, out='bool', _fix=True, series=True, group=False):\n",
    "    \"\"\"\n",
    "    Produce one or more columns with only selected codes\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): Dataframe with events\n",
    "\n",
    "        codes (string, list or dict): The codes for the disease\n",
    "\n",
    "        cols (string, list): Name of columns where codes are located\n",
    "\n",
    "        sep (string, default: None): Separator between codes in same cell (if exist)\n",
    "            (If None, the function will infer the separator)\n",
    "\n",
    "        pid (str, default: 'pid'): Name of column with the personal identification number\n",
    "\n",
    "        codebook (list): User specified list of all possible or allowed codes\n",
    "\n",
    "        merge (bool): Content of all columns is merged to one series # only if out='text'?\n",
    "\n",
    "        group (bool): Star an other notation remain a single group, not split into individual codes\n",
    "\n",
    "        out (string, ['text', 'category', 'bool' or 'int']): Datatype of output column(s)\n",
    "\n",
    "    Notes:\n",
    "        Can produce a set of dummy columns for codes and code groups.\n",
    "        Can also produce a merged column with only extracted codes.\n",
    "        Accept star notation.\n",
    "        Also accepts both single value columns and columns with compound codes and separators\n",
    "        Repeat events in same rows are only extracted once\n",
    "\n",
    "\n",
    "    Example:\n",
    "    to create three dummy columns, based on codes in icdmain column:\n",
    "\n",
    "    >>> extract_codes(df=df,\n",
    "    >>>          codes={'fracture' : 'S72*', 'cd': 'K50*', 'uc': 'K51*'},\n",
    "    >>>          cols=['icdmain', 'icdbi'],\n",
    "    >>>          merge=False,\n",
    "    >>>          out='text')\n",
    "\n",
    "    nb: problem with extract rows if dataframe is empty (none of the requested codes)\n",
    "    \"\"\"\n",
    "    if _fix:\n",
    "        df, cols = _to_df(df=df, cols=cols)\n",
    "        codes, cols, allcodes, sep = _fix_args(df=df, codes=codes, cols=cols, sep=sep, group=group, merge=merge)\n",
    "\n",
    "    subset = pd.DataFrame(index=df.index)\n",
    "\n",
    "    for k, v in codes.items():\n",
    "        rows = get_rows(df=df, codes=v, cols=cols, sep=sep, _fix=False)\n",
    "        if out == 'bool':\n",
    "            subset[k] = rows\n",
    "        elif out == 'int':\n",
    "            subset[k] = rows.astype(int)\n",
    "        elif out == 'category':\n",
    "            subset.loc[rows, k] = k\n",
    "            subset[k] = subset[k].astype('category')\n",
    "        else:\n",
    "            subset[k] = na_rep\n",
    "            subset.loc[rows, k] = k\n",
    "\n",
    "    if (merge) and (out == 'bool'):\n",
    "        subset = subset.astype(int).astype(str)\n",
    "\n",
    "    new_codes = list(subset.columns)\n",
    "\n",
    "    if (merge) and (len(codes) > 1):\n",
    "        headline = ', '.join(new_codes)\n",
    "        merged = subset.iloc[:, 0].str.cat(subset.iloc[:, 1:].values, sep=new_sep,\n",
    "                                           na_rep=na_rep)  # strange .T.values seemed to work previouslyi but it should not have\n",
    "        merged = merged.str.strip(',')\n",
    "        subset = merged\n",
    "        subset.name = headline\n",
    "        if out == 'category':\n",
    "            subset = subset.astype('category')\n",
    "\n",
    "    # return a series if only one code is asked for (and also if merged?)\n",
    "    if series and (len(codes) == 1):\n",
    "        subset = subset.squeeze()\n",
    "\n",
    "    return subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sankey_format(df, labels=None, normalize=False, dropna=False, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Format the dataframe so it is easy fo create a holoviews sankey figure\n",
    "\n",
    "    labels=dict(bio_codes.values())\n",
    "    import holoviews as hv\n",
    "    hv.Sankey(t1).options(label_position='left')\n",
    "    hv.extension('bokeh')\n",
    "    t4=t1.copy()\n",
    "\n",
    "    \"\"\"\n",
    "    a = df\n",
    "    a = a.apply(lambda row: ' '.join(row))\n",
    "    a = a.str.split(expand=True)\n",
    "\n",
    "    a = a.replace(labels)\n",
    "    for col in a.columns:\n",
    "        a[col] = a[col] + ' (' + str(col + 1) + ')'\n",
    "\n",
    "\n",
    "    if not dropna:\n",
    "        a = a.fillna(f'No new')\n",
    "\n",
    "    all_counts = {}\n",
    "    for col in range(len(a.columns))[1:]:\n",
    "        counts = a.groupby(a[col - 1])[col].value_counts(normalize=normalize)\n",
    "        if normalize:\n",
    "            counts = counts.mul(100).astype(int).fillna(0)\n",
    "\n",
    "        counts.name = 'value'\n",
    "        # counts = counts.rename(index=labels).reset_index()\n",
    "        counts = counts.reset_index()\n",
    "        counts.columns = ['source', 'target', 'value']\n",
    "\n",
    "        all_counts[col] = counts\n",
    "    t1 = pd.concat(all_counts, ignore_index=True)\n",
    "\n",
    "    #if normalize:\n",
    "    #    t1['value'] = t1['value'] / t1['value'].sum()\n",
    "\n",
    "    t1 = t1[t1.source != 'No new']\n",
    "\n",
    "    # a.groupby(1)[2].value_counts()\n",
    "    return t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _format_codes(codes, merge=True):\n",
    "    \"\"\"\n",
    "    Makes sure that the codes has the desired format: a dict with strings as\n",
    "    keys (name) and a list of codes as values)\n",
    "\n",
    "    Background: For several functions the user is allower to use strings\n",
    "    when there is only one element in the list, and a list when there is\n",
    "    no code replacement or aggregations, or a dict. To avoid (even more) mess\n",
    "    the input is standardised as soon as possible in a function.\n",
    "\n",
    "    Examples:\n",
    "            codes = '4AB02'\n",
    "            codes='4AB*'\n",
    "            codes = ['4AB02', '4AB04', '4AC*']\n",
    "            codes = ['4AB02', '4AB04']\n",
    "            codes = {'tumor' : 'a4*', 'diabetes': ['d3*', 'd5-d9']}\n",
    "            codes = 'S72*'\n",
    "            codes = ['K50*', 'K51*']\n",
    "\n",
    "            _format_codes(codes, merge=False)\n",
    "\n",
    "    TODO: test for correctness of input, not just reformat (is the key a str?)\n",
    "    \"\"\"\n",
    "    codes = _listify(codes)\n",
    "\n",
    "    # treeatment of pure lists depends on whether special classes should be treated as one merged group or separate codes\n",
    "    # exmple xounting of Z51* could mean count the total number of codes with Z51 OR a shorthand for saying \"count all codes starting with Z51 separately\n",
    "    # The option \"merged, enables the user to switch between these two interpretations\n",
    "\n",
    "    if isinstance(codes, list):\n",
    "        if merge:\n",
    "            codes = {'_'.join(codes): codes}\n",
    "        else:\n",
    "            codes = {code: [code] for code in codes}\n",
    "\n",
    "    elif isinstance(codes, dict):\n",
    "        new_codes = {}\n",
    "        for name, codelist in codes.items():\n",
    "            if isinstance(codelist, str):\n",
    "                codelist = [codelist]\n",
    "            new_codes[name] = codelist\n",
    "        codes = new_codes\n",
    "\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _expand_regex(expr, full_list):\n",
    "    exprs = _listify(expr)\n",
    "\n",
    "    expanded = []\n",
    "\n",
    "    if isinstance(full_list, pd.Series):\n",
    "        pass\n",
    "    elif isinstance(full_list, list):\n",
    "        unique_series = pd.Series(full_list)\n",
    "    elif isinstance(full_list, set):\n",
    "        unique_series = pd.Series(list(full_list))\n",
    "\n",
    "    for expr in exprs:\n",
    "        match = unique_series.str.contains(expr)\n",
    "        expanded.extend(unique_series[match])\n",
    "    return expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _reverse_dict(dikt):\n",
    "    new_dict = {}\n",
    "    for name, codelist in dikt.items():\n",
    "        codelist = _listify(codelist)\n",
    "        new_dict.update({code: name for code in codelist})\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def persons_with(df,\n",
    "                 codes,\n",
    "                 cols,\n",
    "                 pid='pid',\n",
    "                 sep=None,\n",
    "                 merge=True,\n",
    "                 first_date=None,\n",
    "                 last_date=None,\n",
    "                 group=False,\n",
    "                 _fix=True):\n",
    "    \"\"\"\n",
    "    Determine whether people have received a code\n",
    "\n",
    "    Args:\n",
    "        codes (list or dict): codes to mark for\n",
    "            codes to search for\n",
    "                - if list: each code will represent a column\n",
    "                - if dict: the codes in each item will be aggregated to one indicator\n",
    "            cols (str or list of str): Column(s) with the codes\n",
    "            pid (str): colum with the person identifier\n",
    "            first_date (str): use only codes after a given date\n",
    "                the string either represents a date (same for all individuals)\n",
    "                or the name of a column with dates (may be different for different individuals)\n",
    "            last_date (str): only use codes after a given date\n",
    "                the string either represents a date (same for all individuals)\n",
    "                or the name of a column with dates (may be different for different individuals)\n",
    "\n",
    "    Returns:\n",
    "        Series or Dataframe\n",
    "\n",
    "\n",
    "    Examples:\n",
    "        fracture = persons_with(df=df, codes='S72*', cols='icdmain')\n",
    "        fracture = persons_with(df=df, codes={'frac':'S72*'}, cols='icdmain')\n",
    "\n",
    "    Todo:\n",
    "        - function may check if pid_index is unique, in which it does not have to aggregate\n",
    "        - this may apply in general? functions that work on event data may then also work on person level data\n",
    "        - allow user to input person level dataframe source?\n",
    "    \"\"\"\n",
    "    sub = df\n",
    "\n",
    "    if _fix:\n",
    "        df, cols = _to_df(df=df, cols=cols)\n",
    "        codes, cols, allcodes, sep = _fix_args(df=df, codes=codes, cols=cols, sep=sep, merge=merge, group=group)\n",
    "        rows = get_rows(df=df, codes=allcodes, cols=cols, sep=sep, _fix=False)\n",
    "        sub = df[rows]\n",
    "\n",
    "    df_persons = sub.groupby(pid)[cols].apply(lambda s: pd.unique(s.values.ravel()).tolist()).astype(str)\n",
    "\n",
    "    # alternative approach, also good, and avoids creaintg personal dataframe\n",
    "    # but ... regeis is fast since it stopw when it finds one true code!\n",
    "    #    c=df.icdbi.str.split(', ', expand=True).to_sparse()\n",
    "    #    c.isin(['S720', 'I10']).any(axis=1).any(level=0)\n",
    "\n",
    "    persondf = pd.DataFrame(index=df[pid].unique().tolist())\n",
    "    for name, codes in codes.items():\n",
    "        codes_regex = '|'.join(codes)\n",
    "        persondf[name] = df_persons.str.contains(codes_regex, na=False)\n",
    "\n",
    "    return persondf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def stringify_durations(df,\n",
    "                        codes=None,\n",
    "                        cols=None,\n",
    "                        pid='pid',\n",
    "                        step=120,\n",
    "                        sep=None,\n",
    "\n",
    "                        event_start='in_date',\n",
    "                        event_end=None,\n",
    "                        event_duration='ddd',\n",
    "\n",
    "                        first_date=None,\n",
    "                        last_date=None,\n",
    "                        censored_date=None,\n",
    "\n",
    "                        ncodes=None,\n",
    "\n",
    "                        no_event='-',\n",
    "                        time_sep='|',\n",
    "\n",
    "                        merge=True,\n",
    "                        info=None,\n",
    "                        report=False):\n",
    "    \"\"\"\n",
    "    Creates a string for each individual describing the time duration selected code events (example: a-, ad, --, a)\n",
    "\n",
    "    Args:\n",
    "        df: dataframe\n",
    "        codes: codes to be used to mark an event\n",
    "        cols: columns with the event codes\n",
    "        pid: column with the personal identification number\n",
    "        event_start: column containing the date for the event\n",
    "        sep: the separator used between events if a column has multiple events in a cell\n",
    "        keep_repeats: identical events after each other are reduced to one (if true)\n",
    "        only_unique: deletes all events that have occurred previously for the individual (if true)\n",
    "\n",
    "    Returns:\n",
    "        series with a string that describes the events for each individual\n",
    "    Example:\n",
    "\n",
    "    >>> codes={'i' : ['4AB02', 'L04AB02'], 'a': ['4AB04','L04AB04']}\n",
    "    >>> events=sa.stringify_durations(df=mdf, codes=codes, cols='codes',\n",
    "    event_start='date', first_date=None, sep=',', merge=True)\n",
    "\n",
    "    >>> codes={'i' : ['4AB02', 'L04AB02'], 'a': ['4AB04','L04AB04']}\n",
    "    >>> codes={'i' : ['L04*'], 'b': ['4AB04','L04AB04']}\n",
    "\n",
    "\n",
    "    >>> codes = {'i':'L01BB02 L04AX03 L01BA01 L04AD01 L04AD02 L04AA06'.split(),\n",
    "                 'b':'L04AB02 L04AB04 L04AB06 L04AA33 L04AC05 L04AA23'.split()}\n",
    "\n",
    "\n",
    "    >>> events=sa.stringify_durations(df=mdf, codes=codes, cols='codes',\n",
    "    event_start='date', first_date=None, sep=',', merge=False, step=100)\n",
    "\n",
    "    >>> codes={'L04A*' : 'i', 'L04AB*' : 'a', 'H02*' : 'c'}\n",
    "    >>> pr=pr.set_index('pid_index')\n",
    "    >>> pr['first_date'] = pr.groupby('pid')['date'].min()\n",
    "    >>> events=stringify_durations(df=df, codes=codes, col='ncmpalt', start='start_date', first_date='first', dataset_end_date=\"01-01-2018\")\n",
    "\n",
    "\n",
    "    background\n",
    "        to identify treatment patters, first stringify each treatment,\n",
    "        then aggregate the different treatments to one string\n",
    "        each \"cell\" in the string (separated by sep) represent one time unit\n",
    "        the time unit can be further aggregated to reduce the level of detail\n",
    "\n",
    "    example output (one such row for each person)\n",
    "        a---s, a---, ai-s, a---, ----\n",
    "\n",
    "        Interpretation: A person with event a and s in first time perod, then a only in second,\n",
    "        the a, i and s in the third, a only in fourth and no events in the last\n",
    "\n",
    "    purpose\n",
    "        examine typical treatment patterns and correlations\n",
    "        use regex or other string operations on this to get statistcs\n",
    "        (time on first line of treatment, number of switches, stops)\n",
    "\n",
    "    \"\"\"\n",
    "    # drop rows with missing observations in required variables\n",
    "\n",
    "    if report:\n",
    "        obs = len(df)\n",
    "        npid = df[pid].nunique()\n",
    "        if isinstance(codes, dict):\n",
    "            allcodes = _get_allcodes(codes)\n",
    "        elif isinstance(codes, str):\n",
    "            allcodes = _listify(codes)\n",
    "        # todo: also possible notational codes! better eliminate this?\n",
    "        rows = get_rows(df=df, codes=allcodes, cols=cols, sep=sep)\n",
    "        code_obs = len(df[rows])\n",
    "        code_npid = df[rows][pid].nunique()\n",
    "\n",
    "    df = df.dropna(subset=[pid, event_start])  # also drop if codes is missing\n",
    "\n",
    "    if event_end:\n",
    "        df = df.dropna(subset=[event_end])\n",
    "    elif event_duration:\n",
    "        df = df.dropna(subset=[event_duration])\n",
    "        if df[event_duration].min() < 0:\n",
    "            print('Error: The specified duration column contains negative values. They are dropped')\n",
    "            df = df[df[event_duration] >= 0]\n",
    "    else:\n",
    "        print('Error: Either event_end or event_duration has to be specified.')\n",
    "\n",
    "    # find default min and max dates\n",
    "    # will be used as starting points for the string\n",
    "    # if first_date and last_date are not specified\n",
    "    min_date = df[event_start].min()\n",
    "    max_date = df[event_start].max()\n",
    "\n",
    "    # drop rows outside specified time period of interest\n",
    "    if first_date:\n",
    "        if first_date in df.columns:\n",
    "            df = df[df[event_start] >= df[first_date]]\n",
    "        elif isinstance(first_date, dict):\n",
    "            pass\n",
    "        else:\n",
    "            # if first_date is not a column name, it is assumed to be a date\n",
    "            try:\n",
    "                min_date = pd.to_datetime(first_date)\n",
    "                df = df[df[event_start] >= min_date]\n",
    "            except:\n",
    "                print(\n",
    "                    'Error: The first_date argument has to be on of: None, a dict, a column name or a string that represents a date')\n",
    "\n",
    "    if last_date:\n",
    "        if last_date in df.columns:\n",
    "            df = df[df[event_start] >= df[last_date]]\n",
    "        elif isinstance(last_date, dict):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                max_date = pd.to_datetime(last_date)\n",
    "                df = df[df[event_start] <= max_date]\n",
    "            except:\n",
    "                print(\n",
    "                    'Error: The last_date argument has to be on of: None, a dict, a column name or a string the represents a date')\n",
    "\n",
    "    # note an individual min date cannot be before overall specified min date\n",
    "    # should raise error if user tries this\n",
    "    # same with max: individual cannot be larger than overall\n",
    "\n",
    "    max_length_days = (max_date - min_date).days\n",
    "    max_length_steps = int(max_length_days / step)\n",
    "\n",
    "    # if codes are not specified, use the five most common codes\n",
    "    if not codes:\n",
    "        cols = _expand_cols(_listify(cols))\n",
    "        if not ncodes: ncodes = 4\n",
    "        codes = count_codes(df=df, cols=cols, sep=sep).sort_values(ascending=False)[:ncodes]\n",
    "\n",
    "    # fix formatting of input (make list out of a string input and so on)\n",
    "    codes, cols, allcodes, sep = _fix_args(df=df, codes=codes, cols=cols, sep=sep)\n",
    "\n",
    "    # get the rows that contain the relevant codes\n",
    "    rows = get_rows(df=df, codes=allcodes, cols=cols, sep=sep, _fix=False)\n",
    "    subset = df[rows].copy()  # maybe use .copy to avoid warnings? but takes time and memory\n",
    "    subset = subset.set_index(pid, drop=False)\n",
    "    subset.index.name = 'pid_index'\n",
    "    subset = subset.sort_values([pid, event_start])\n",
    "\n",
    "    if report:\n",
    "        sub_obs = len(subset)\n",
    "        sub_npid = subset[pid].nunique()\n",
    "\n",
    "    # find start and end position of each event (number of steps from overall min_date)\n",
    "    # to do: do not use those column names (may overwrite original names), use uuid names?\n",
    "    subset['start_position'] = (subset[event_start] - min_date).dt.days.div(step).astype(int)\n",
    "\n",
    "    if event_end:\n",
    "        subset['end_position'] = (subset[event_end] - min_date).dt.days.div(step).astype(int)\n",
    "    elif event_duration:\n",
    "        subset['end_date'] = subset[event_start] + pd.to_timedelta(subset[event_duration].astype(int), unit='D')\n",
    "        subset['end_position'] = (subset['end_date'] - min_date).dt.days.div(step).astype(int)\n",
    "\n",
    "    # to do: may allow duration dict?\n",
    "    # for instance: some drugs last 15 days, some drugs last 25 days . all specified in a dict\n",
    "\n",
    "    # create series with only the relevant codes for each person and position\n",
    "    code_series = extract_codes(df=subset.set_index([pid, 'start_position', 'end_position']),\n",
    "                                codes=codes,\n",
    "                                cols=cols,\n",
    "                                sep=sep,\n",
    "                                new_sep=',',\n",
    "                                merge=False,\n",
    "                                out='text',\n",
    "                                _fix=False)\n",
    "\n",
    "    unique_codes = list(code_series.columns)\n",
    "\n",
    "    code_series = pd.melt(code_series.reset_index(),\n",
    "                          id_vars=['pid', 'start_position', 'end_position'],\n",
    "                          value_vars=unique_codes)\n",
    "\n",
    "    # drop duplicates (same type of even in same period for same individual)\n",
    "    code_series = code_series.drop_duplicates().set_index(pid, drop=False)\n",
    "    code_series.index.name = 'pid_index'\n",
    "    ## make dict with string start and end positions for each individual\n",
    "    # explanation:\n",
    "    # the string is first made marking events in positions using calendar time\n",
    "    # but often we want the end result to be strings that start at specified\n",
    "    # individual dates, and not the same calendar date for all\n",
    "    # for instance it is often useful to start the string at the date the\n",
    "    # person receives a diagnosis\n",
    "    # same with end of string: strings may end when a patient dies\n",
    "    # user can specify start and end dates by pointing to columns with dates\n",
    "    # or they may specify an overall start and end date\n",
    "    # if individual dates are specified, the long string based on calendar\n",
    "    # time is sliced to include only the relevant events\n",
    "\n",
    "    if first_date:\n",
    "        # if a column is specified\n",
    "        if first_date in subset.columns:\n",
    "            start_date = subset.groupby(pid)[first_date].first().dropna().to_dict()\n",
    "        # do nothing if a dict mapping pids to last_dates is already specified\n",
    "        elif isinstance(first_date, dict):\n",
    "            pass\n",
    "        # if a single overall date is specified\n",
    "        else:\n",
    "            date = pd.to_datetime(first_date)\n",
    "            start_date = {pid: date for pid in subset[pid].unique()}\n",
    "        # convert start date to start position in string\n",
    "        string_start_position = {pid: int((date - min_date).days / step)\n",
    "                                 for pid, date in start_date.items()}\n",
    "\n",
    "    if last_date:\n",
    "        if last_date in subset:\n",
    "            end_date = subset.groupby(pid)[last_date].first().dropna().to_dict()\n",
    "        # do nothing if a dict mapping pids to last_dates is already specified\n",
    "        elif isinstance(last_date, dict):\n",
    "            pass\n",
    "        else:\n",
    "            date = pd.to_datetime(last_date)\n",
    "            end_date = {pid: date for pid in subset[pid].unique()}\n",
    "        # convert date to position in string\n",
    "        string_end_position = {pid: (date - min_date).dt.days.div(step).astype(int)\n",
    "                               for pid, date in end_date.items()}\n",
    "\n",
    "        # takes dataframe for an individual and makes a string with the events\n",
    "\n",
    "    def make_string(events):\n",
    "        # get pid of individual (required to find correct start and end point)\n",
    "        person = events.index[0]\n",
    "\n",
    "        # make a list of maximal length with no events\n",
    "        event_list = [no_event] * (max_length_steps + 1)\n",
    "\n",
    "        from_to_positions = tuple(zip(events['start_position'].tolist(), events['end_position'].tolist()))\n",
    "\n",
    "        # loop over all events the individual has and put code in correct pos.\n",
    "        for pos in from_to_positions:\n",
    "            event_list[pos[0]:pos[1]] = code\n",
    "        event_string = \"\".join(event_list)\n",
    "\n",
    "        # slice to correct start and end of string (if specified)\n",
    "        if first_date:\n",
    "            event_string = event_string[string_start_position[person]:]\n",
    "        if last_date:\n",
    "            max_position = int((max_date - min_date).days / step)\n",
    "            event_string = event_string[:-(max_position - string_end_position[person])]\n",
    "        return event_string\n",
    "\n",
    "    # new dataframe to store each string for each individual for each code\n",
    "    string_df = pd.DataFrame(index=code_series[pid].unique())\n",
    "    string_df.index.name = 'pid_index'\n",
    "\n",
    "    # loop over each code, aggregate strong for each individual, store in df\n",
    "    for code in unique_codes:\n",
    "        code_df = code_series[code_series['value'].isin([code])] # maybe == is better (safer bco compounds + faster?)\n",
    "        stringified = code_df.groupby(pid, sort=False).apply(make_string)\n",
    "        string_df[code] = stringified\n",
    "\n",
    "    if merge:\n",
    "        string_df = interleave_strings(string_df, no_event=no_event, time_sep=time_sep)\n",
    "\n",
    "    if report:\n",
    "        final_obs = len(subset)\n",
    "        final_npid = len(string_df)\n",
    "        print(f\"\"\"\n",
    "                                     events,  unique ids\n",
    "              Original dataframe     {obs}, {npid} \n",
    "              Filter codes           {code_obs}, {code_npid}\n",
    "              Filter missing         {sub_obs}, {sub_npid}\n",
    "              Final result:          {final_obs}, {final_npid}\"\"\")\n",
    "    return string_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _make_binary(df, cols=None, no_event=' ', time_sep='|', pad=False):\n",
    "    if isinstance(df, pd.Series):\n",
    "        name = df[col].name\n",
    "        df=df.str.replace(no_event, '0')\n",
    "        df=df.str.replace(name, '1')\n",
    "    else:\n",
    "        # if no cols are selected, use all cols\n",
    "        if not cols:\n",
    "            cols = list(df.columns)\n",
    "        # replace event chars with 1 and no events with 0\n",
    "        for col in cols:\n",
    "            name = df[col].name\n",
    "            df[col]=df[col].str.replace(no_event, '0')\n",
    "            df[col]=df[col].str.replace(name, '1')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def interleave_strings(df, cols=None, time_sep=\"|\", no_event=' ', agg=False):\n",
    "    \"\"\"\n",
    "    Interleaves strings in two or more columns\n",
    "\n",
    "    parameters\n",
    "        cols : list of columns with strings to be interleaved\n",
    "        nan : value to be used in place of missing values\n",
    "        sep : seperator to be used between time periods\n",
    "        agg : numeric, used to indicate aggregation of time scale\n",
    "                default is 1\n",
    "\n",
    "    background\n",
    "        to identify treatment patters, first stringify each treatment,\n",
    "        then aggregate the different treatments to one string\n",
    "        each \"cell\" in the string (separated by sep) represent one time unit\n",
    "        the time unit can be further aggregated to reduce the level of detail\n",
    "\n",
    "    example output (one such row for each person)\n",
    "        a---s, a---, ai-s, a---, ----\n",
    "\n",
    "        Interpretation: A person with event a and s in first time perod, then a only in second,\n",
    "        the a, i and s in the third, a only in fourth and no events in the last\n",
    "\n",
    "    purpose\n",
    "        examine typical treatment patterns and correlations\n",
    "        use regex or other string operations on this to get statistcs\n",
    "        (time on first line of treatment, number of switches, stops)\n",
    "\n",
    "    \"\"\"\n",
    "    # if cols is not specified, use all columns in dataframe\n",
    "    if not cols:\n",
    "        cols = list(df.columns)\n",
    "\n",
    "    if agg:\n",
    "        for col in cols:\n",
    "            df[col] = df[col].fillna(no_event)\n",
    "            # find event symbol, imply check if all are missing, no events\n",
    "            try:\n",
    "                char = df[col].str.cat().strip().str.strip('-')[0]  # improvable?\n",
    "            except:\n",
    "                df[col] = (col.str.len() / agg) * no_event\n",
    "\n",
    "            def aggregator(text, agg):\n",
    "                missing = no_event * agg\n",
    "                units = (text[i:i + agg] for i in range(0, len(text), agg))\n",
    "                new_aggregated = (no_event if unit == missing else char for unit in units)\n",
    "                new_str = \"\".join(new_aggregated)\n",
    "                return new_str\n",
    "        df[col] = df[col].apply(aggregator, agg=agg)\n",
    "\n",
    "    if time_sep:\n",
    "        interleaved = df[cols].fillna(no_event).apply(\n",
    "            (lambda x: time_sep.join(\n",
    "                \"\".join(i)\n",
    "                for i in zip_longest(*x, fillvalue=no_event))),\n",
    "            axis=1)\n",
    "    else:\n",
    "        interleaved = df[cols].fillna('-').apply(\n",
    "            (lambda x: \"\".join(chain(*zip_longest(*x, fillvalue=no_event)))),\n",
    "            axis=1)\n",
    "\n",
    "    return interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def left_justify(s, fill=' '):\n",
    "    \"\"\"\n",
    "    after stringify, to make events at same time be in same position\n",
    "    and no, not as crucial as left-pad!\n",
    "    \"\"\"\n",
    "    nmax = s.apply(len).max()\n",
    "    s = s.str.pad(width=nmax, side='right', fillchar=fill)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def overlay_strings(df, cols=None, sep=\",\", nan='-', collisions='x', interleaved=False):\n",
    "    \"\"\"\n",
    "    overlays strings from two or more columns\n",
    "\n",
    "    note\n",
    "        most useful when aggregating a string for events that usually do not happen in the same time frame\n",
    "\n",
    "    parameters\n",
    "        cols : list of columns with strings to be interleaved\n",
    "        nan : value to be used in place of missing values\n",
    "        collisions: value to be used if there is a collision between events in a position\n",
    "\n",
    "\n",
    "    background\n",
    "        to identify treatment patters, first stringify each treatment,\n",
    "        then aggregate the different treatments to one string\n",
    "        each \"cell\" in the string (separated by sep) represent one time unit\n",
    "        the time unit can be further aggregated to reduce the level of detail\n",
    "\n",
    "    example output (one such row for each person)\n",
    "        asaaa--s--aa-s-a\n",
    "\n",
    "        Interpretation: A person with event a and s in first time perod, then a only in second,\n",
    "        the a, i and s in the third, a only in fourth and no events in the last\n",
    "\n",
    "    purpose\n",
    "        examine typical treatment patterns and correlations\n",
    "        use regex or other string operations on this to get statistcs\n",
    "        (time on first line of treatment, number of switches, stops)\n",
    "\n",
    "    todo\n",
    "        more advanced handling of collisions\n",
    "            - special symbols for different types of collisions\n",
    "            - warnings (and keep/give info on amount and type of collisions)\n",
    "\n",
    "    \"\"\"\n",
    "    # if cols is not specified, use all columns in dataframe\n",
    "    if not cols:\n",
    "        cols = list(df.columns)\n",
    "\n",
    "    interleaved = df[cols].fillna('-').apply(\n",
    "        (lambda x: \"\".join(chain(*zip_longest(*x, fillvalue='-')))),\n",
    "        axis=1)\n",
    "    step_length = len(cols)\n",
    "\n",
    "    def event_or_collision(events):\n",
    "        try:\n",
    "            char = events.strip('-')[0]\n",
    "        except:\n",
    "            char = '-'\n",
    "        n = len(set(events).remove('-'))\n",
    "        if n > 1:\n",
    "            char = 'x'\n",
    "        return char\n",
    "\n",
    "    def overlay_individuals(events):\n",
    "\n",
    "        units = (events[i:i + step_length] for i in range(0, len(events), step_length))\n",
    "\n",
    "        new_aggregated = (event_or_collision(unit) for unit in units)\n",
    "        new_str = \"\".join(new_aggregated)\n",
    "        return new_str\n",
    "\n",
    "    interleaved.apply(overlay_individuals)\n",
    "\n",
    "    return interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def shorten(events, agg=3, no_event=' '):\n",
    "    \"\"\"\n",
    "    create a new and shorter string with a longer time step\n",
    "\n",
    "    parameters\n",
    "        events: (str) string of events that will be aggregated\n",
    "        agg: (int) the level of aggregation (2=double the step_length, 3=triple)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        char = events.strip(no_event)[0]\n",
    "    except:\n",
    "        char = no_event\n",
    "    units = (events[i:i + agg] for i in range(0, len(events), agg))\n",
    "    new_aggregated = (no_event if unit == no_event else char for unit in units)\n",
    "    new_str = \"\".join(new_aggregated)\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def shorten_interleaved(text, agg=3, time_sep=',', no_event=' '):\n",
    "    \"\"\"\n",
    "    text=\"a-si,a--i,a-s-,--si,---i,--s-\"\n",
    "\n",
    "    shorten_interleaved(c, agg=2)\n",
    "\n",
    "    the original string must have a distinction between time_sep and no_event_sep\n",
    "    (if not, could try to infer)\n",
    "    \"\"\"\n",
    "    units = text.split(time_sep)\n",
    "    ncodes = len(units[0])\n",
    "    nunits = len(units)\n",
    "\n",
    "    unitlist = [units[i:i + agg] for i in range(0, nunits, agg)]\n",
    "    charlist = [\"\".join(aggunit) for aggunit in unitlist]\n",
    "    unique_char = [\"\".join(set(chain(chars))) for chars in charlist]\n",
    "    new_str = time_sep.join(unique_char)\n",
    "    # ordered or sorted?\n",
    "    # delete last if it is not full ie. not as many timee units in it as the others?\n",
    "    # shortcut for all\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def stringify_order(df, codes=None, cols=None, pid='pid', event_start='date',\n",
    "                    sep=None, time_sep='', first_date=None, last_date=None, period=None, keep_repeats=True,\n",
    "                    only_unique=False, _fix=True):\n",
    "    \"\"\"\n",
    "    Creates a string for each individual describing selected code events in the order they occurred\n",
    "\n",
    "    Args:\n",
    "        df: dataframe\n",
    "        codes: codes to be used to mark an event\n",
    "        cols: columns with the event codes\n",
    "        pid: column with the personal identification number\n",
    "        event_start: column containing the date for the event\n",
    "        sep: the separator used between events if a column has multiple events in a cell\n",
    "        keep_repeats: identical events after each other are reduced to one (if true)\n",
    "        only_unique: deletes all events that have occurred previously for the individual (if true)\n",
    "\n",
    "    Returns:\n",
    "        series with a string that describes the events for each individual\n",
    "\n",
    "\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    >>> bio_codes= {'L04AA23': 'n', 'L04AA33': 'v', 'L04AB02': 'i', 'L04AB04': 'a','L04AB06': 'g', 'L04AC05': 'u'}\n",
    "\n",
    "    >>> bio_codes={'e' : '4AB01', 'i' : '4AB02', 'a' : '4AB04'}\n",
    "\n",
    "    >>> bio_codes={'i' : '4AB02', 'a' : '4AB04'}\n",
    "\n",
    "    >>> bio_codes= {'n': ['L04AA23', '4AA23'],\n",
    "                    'v': ['L04AA33', '4AA33'],\n",
    "                    'i': ['L04AB02', '4AB02'],\n",
    "                    'a': ['L04AB04', '4AB04'],\n",
    "                    'g': ['L04AB06', '4AB06'],\n",
    "                    'u': ['L04AC05', '4AC05']}\n",
    "\n",
    "\n",
    "    >>> a=stringify_order(df=df, codes=bio_codes, cols='ncmpalt', pid='pid', event_start='start_date', sep=',', keep_repeats=True, only_unique=False)\n",
    "\n",
    "    >>> a=sa.stringify_order(df=mdf, codes=bio_codes, cols='codes', pid='pid', first_date='first_ibd',\n",
    "    event_start='date', sep=',', keep_repeats=False, only_unique=False, time_sep='', period=700)\n",
    "\n",
    "\n",
    "    >>> bio_rows=get_rows(df=pr, codes=list(codes.keys()), cols='atc')\n",
    "    >>> pr['first_bio']=pr[bio_rows].groupby('pid')['date'].min()\n",
    "\n",
    "    >>> stringify_order(df=pr, codes=codes, cols='atc', pid='pid', event_date='date', sep=',')\n",
    "\n",
    "    >>> stringify_order(df=pr, codes=bio_codes, cols='codes', pid='pid', event_date='date', sep=',')\n",
    "\n",
    "\n",
    "    background\n",
    "        to identify treatment patters, first stringify each treatment,\n",
    "        then aggregate the different treatments to one string\n",
    "        each \"cell\" in the string (separated by sep) represent one time unit\n",
    "        the time unit can be further aggregated to reduce the level of detail\n",
    "\n",
    "    example output (one such row for each person)\n",
    "        a---s, a---, ai-s, a---, ----\n",
    "\n",
    "        Interpretation: A person with event a and s in first time perod, then a only in second,\n",
    "        the a, i and s in the third, a only in fourth and no events in the last\n",
    "\n",
    "    purpose\n",
    "        examine typical treatment patterns and correlations\n",
    "        use regex or other string operations on this to get statistcs\n",
    "        (time on first line of treatment, number of switches, stops)\n",
    "    \"\"\"\n",
    "\n",
    "    df.index.name = 'pid_index'  # avoid errors, and yes, require pid to be in index (?)\n",
    "\n",
    "    df = df.dropna(subset=[pid, event_start])\n",
    "\n",
    "    if first_date:\n",
    "        df = df.dropna(subset=[first_date])\n",
    "\n",
    "        # if a column is specified\n",
    "        if first_date in df.columns:\n",
    "            include = (df[event_start] >= df[first_date])\n",
    "            # if a single overall date is specified\n",
    "        else:\n",
    "            date = pd.to_datetime(first_date)\n",
    "            include = (df[event_start] >= date)\n",
    "        df = df[include]\n",
    "\n",
    "    if last_date:\n",
    "        df = df.dropna(subset=[last_date])\n",
    "\n",
    "        if last_date in df.columns:\n",
    "            include = (df[event_start] <= df[last_date])\n",
    "        else:\n",
    "            date = pd.to_datetime(last_date)\n",
    "            include = (df[event_start] <= df[last_date])\n",
    "        df = df[include]\n",
    "\n",
    "    # period represents the days from the first_date to be included\n",
    "    # cannot specify both period and last_date(?)\n",
    "    if period:\n",
    "        if first_date:\n",
    "            end_date = df[first_date] + pd.to_timedelta(period, unit='D')\n",
    "            include = (df[event_start] <= end_date)\n",
    "        else:\n",
    "            time_after = (df[event_start] - df.groupby(pid)[event_start].min()) / np.timedelta64(1, 'D')\n",
    "            include = (time_after <= period).values  # strange need this, tries to reindex if not\n",
    "        df = df[include]\n",
    "\n",
    "    # fix formatting of input\n",
    "    if _fix:\n",
    "        df, cols = _to_df(df=df, cols=cols)\n",
    "        codes, cols, allcodes, sep = _fix_args(df=df, codes=codes, cols=cols, sep=sep)\n",
    "    else:\n",
    "        allcodes=_get_allcodes(codes)\n",
    "\n",
    "    # get the rows with the relevant columns\n",
    "    rows = get_rows(df=df, codes=allcodes, cols=cols, sep=sep, _fix=False)\n",
    "    subset = df[rows]  # do I need to copy?\n",
    "    subset.index.name = 'pid_index'\n",
    "    subset = subset.sort_values(by=[pid, event_start]).set_index('pid')\n",
    "\n",
    "    # extract relevant codes and aggregate for each person\n",
    "    code_series = extract_codes(df=subset, codes=codes, cols=cols, sep=sep, new_sep='', merge=True, out='text',\n",
    "                                _fix=False)\n",
    "    #    if isinstance(code_series, pd.DataFrame):\n",
    "    #        code_series = pd.Series(code_series)\n",
    "    string_df = code_series.groupby(level=0).apply(lambda codes: codes.str.cat(sep=time_sep))\n",
    "\n",
    "    # eliminate repeats in string\n",
    "    if not keep_repeats:\n",
    "        string_df = string_df.str.replace(r'([a-z])\\1+', r'\\1')\n",
    "\n",
    "    if only_unique:\n",
    "        def uniqify(text):\n",
    "            while re.search(r'([a-z])(.*)\\1', text):\n",
    "                text = re.sub(r'([a-z])(.*)\\1', r'\\1\\2', text)\n",
    "            return text\n",
    "\n",
    "        string_df = string_df.apply(uniqify)\n",
    "    return string_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def del_repeats(str_series):\n",
    "    \"\"\"\n",
    "    deletes consecutively repeated characters from the strings in a series\n",
    "\n",
    "    \"\"\"\n",
    "    no_repeats = str_series.str.replace(r'([a-z])\\1+', r'\\1')\n",
    "    return no_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def del_singles(text):\n",
    "    \"\"\"\n",
    "    Deletes single characters from string\n",
    "    todo: how to deal with first and last position ... delete it too?\n",
    "\n",
    "    \"\"\"\n",
    "    # text with only one character are by definition singles\n",
    "    if len(text) < 2:\n",
    "        no_singles = ''\n",
    "    else:\n",
    "        no_singles = \"\".join([letter for n, letter in enumerate(text[1:-1], start=1) if\n",
    "                              ((text[n - 1] == letter) or (text[n + 1] == letter))])\n",
    "        # long textx may not have any singles, so check before continue\n",
    "        if len(no_singles) < 1:\n",
    "            no_singles = ''\n",
    "        else:\n",
    "            if text[0] == no_singles[0]:\n",
    "                no_singles = text[0] + no_singles\n",
    "            if text[-1] == no_singles[-1]:\n",
    "                no_singles = no_singles + text[-1]\n",
    "\n",
    "    return no_singles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def stringify_time(df,\n",
    "                   codes=None,\n",
    "                   cols=None,\n",
    "                   pid='pid',\n",
    "                   sep=None,\n",
    "                   step=90,\n",
    "\n",
    "                   event_start='date',  # use start end\n",
    "                   nfirst=None,  # ncodes\n",
    "\n",
    "                   first_date=None,\n",
    "                   # use just first, last, censored. Accept integers to indicate period/days relative to the start date\n",
    "                   last_date=None,\n",
    "                   censored_date=None,\n",
    "\n",
    "                   time_sep='|',\n",
    "                   no_event=' ',\n",
    "                   collision='*',\n",
    "\n",
    "                   merge=True,\n",
    "                   info=None):\n",
    "    \"\"\"\n",
    "    Creates a string for each individual describing events at position in time\n",
    "\n",
    "    Args:\n",
    "        df: dataframe\n",
    "        codes: codes to be used to mark an event\n",
    "        cols: columns with the event codes\n",
    "        pid: column with the personal identification number\n",
    "        event_start: column containing the date for the event\n",
    "        sep: the seperator used between events if a column has multiple events in a cell\n",
    "        keep_repeats: identical events after each other are reduced to one (if true)\n",
    "        only_unique: deletes all events that have occurred previously for the individual (if true)\n",
    "\n",
    "    Returns:\n",
    "        series with a string that describes the events for each individual\n",
    "\n",
    "    Example:\n",
    "        codes={'i': '4AB02', 'a':'4AB04'}\n",
    "        codes={'i': ['4AB02','L04AB02'], 'a': ['4AB04', 'L04AB04'], 'e':['4AB01']}\n",
    "\n",
    "\n",
    "        df['diagnosis_date']=df[df.icdmain.fillna('').str.contains('K50|K51')].groupby('pid')['start_date'].min()\n",
    "\n",
    "    a=stringify_time(df=mdf,  codes=codes, cols='codes', pid='pid', event_start='date',\n",
    "    first_date='first_ibd', step=90, sep=',', no_event=' ', time_sep=' ')\n",
    "\n",
    "\n",
    "    background\n",
    "        to identify treatment patters, first stringify each treatment,\n",
    "        then aggregate the different treatments to one string\n",
    "        each \"cell\" in the string (separated by sep) represent one time unit\n",
    "        the time unit can be further aggregated to reduce the level of detail\n",
    "\n",
    "    example output (one such row for each person)\n",
    "        a---s, a---, ai-s, a---, ----\n",
    "\n",
    "        Interpretation: A person with event a and s in first time perod, then a only in second,\n",
    "        the a, i and s in the third, a only in fourth and no events in the last\n",
    "\n",
    "    purpose\n",
    "        examine typical treatment patterns and correlations\n",
    "        use regex or other string operations on this to get statistcs\n",
    "        (time on first line of treatment, number of switches, stops)\n",
    "    \"\"\"\n",
    "\n",
    "    # drop rows with missing observations in required variables\n",
    "    df = df.dropna(subset=[pid, event_start])\n",
    "\n",
    "    # find default min and max dates to be used if not user specified\n",
    "    min_date = df[event_start].min()\n",
    "    max_date = df[event_start].max()\n",
    "\n",
    "    # drop rows outside time period of interest\n",
    "    if first_date:\n",
    "        if first_date in df.columns:\n",
    "            df = df[df[event_start] >= df[first_date]]\n",
    "        else:\n",
    "            min_date = pd.to_datetime(first_date)\n",
    "            df = df[df[event_start] >= min_date]\n",
    "\n",
    "    if last_date:\n",
    "        if last_date in df.columns:\n",
    "            df = df[df[event_start] >= df[last_date]]\n",
    "        else:\n",
    "            max_date = pd.to_datetime(last_date)\n",
    "            df = df[df[event_start] <= max_date]\n",
    "\n",
    "    # note an individual min date cannot be before overall specified min date\n",
    "    # should raise error if user tries this\n",
    "    # same with max: individual cannot be larger than overall\n",
    "\n",
    "    max_length_days = (max_date - min_date).days\n",
    "    max_length_steps = int(max_length_days / step)\n",
    "\n",
    "    # if codes or nfirst are not specified, use the five most common codes\n",
    "    if not codes:\n",
    "        cols = _expand_cols(_listify(cols))\n",
    "        if not nfirst: nfirst = 5\n",
    "        codes = count_codes(df=df, cols=cols, sep=sep).sort_values(ascending=False)[:nfirst]\n",
    "\n",
    "    # fix formatting of input (make list out of a string input and so on)\n",
    "    codes, cols, allcodes, sep = _fix_args(df=df, codes=codes, cols=cols, sep=sep)\n",
    "\n",
    "    # get the rows that contain the relevant codes\n",
    "    rows = get_rows(df=df, codes=allcodes, cols=cols, sep=sep, _fix=False)\n",
    "    subset = df[rows].copy()  # maybe use .copy to avoid warnings?\n",
    "    subset.index.name = 'pid_index'\n",
    "\n",
    "    # find position of each event (number of steps from overall min_date)\n",
    "    subset['position'] = (subset[event_start] - min_date).dt.days.div(step).astype(int)\n",
    "\n",
    "    subset = subset.sort_values(by=[pid, 'position']).set_index([pid, 'position'])\n",
    "\n",
    "    # create series with only the relevant codes for each person and position\n",
    "    code_series = extract_codes(df=subset,\n",
    "                                codes=codes,\n",
    "                                cols=cols,\n",
    "                                sep=sep,\n",
    "                                new_sep=',',\n",
    "                                merge=True,\n",
    "                                out='text',\n",
    "                                _fix=False)\n",
    "\n",
    "    # base further aggregation on the new extracted series with its col and codes\n",
    "    col = code_series.name\n",
    "    codes = code_series.name.split(', ')\n",
    "\n",
    "    # drop duplicates (same type of even in same period for same individual)\n",
    "    code_series = code_series.reset_index().drop_duplicates().set_index(pid, drop=False)\n",
    "    code_series.index.name = 'pid_index'\n",
    "\n",
    "    ## make dict with string start end end positions for each individual\n",
    "    # explanation:\n",
    "    # the string is first made marking events in positions using calendar time\n",
    "    # but often we want the end result to be strings that start at specified\n",
    "    # individual dates, and not the same calendar date for all\n",
    "    # for instance it is often useful to start the string at the date the\n",
    "    # person receives a diagnosis\n",
    "    # same with end of string: strings may end when a patient dies\n",
    "    # user can specify start and end dates by pointing to columns with dates\n",
    "    # or they may specify an overall start and end date\n",
    "    # if individual dates are specified, the long string based on calendar\n",
    "    # time is sliced to include only the relevant events\n",
    "\n",
    "    if first_date:\n",
    "        # if a column is specified\n",
    "        if first_date in subset.columns:\n",
    "            start_date = subset.groupby(pid)[first_date].first().dropna().to_dict()\n",
    "        # if a single overall date is specified\n",
    "        else:\n",
    "            date = pd.to_datetime(first_date)\n",
    "            start_date = {pid: date for pid in subset[pid].unique()}\n",
    "        # convert start date to start position in string\n",
    "        start_position = {pid: int((date - min_date).days / step)\n",
    "                          for pid, date in start_date.items()}\n",
    "\n",
    "    if last_date:\n",
    "        if last_date in subset:\n",
    "            end_date = subset.groupby(pid)[last_date].first().dropna().to_dict()\n",
    "        else:\n",
    "            date = pd.to_datetime(last_date)\n",
    "            end_date = {pid: date for pid in subset[pid].unique()}\n",
    "        # convert date to position in string\n",
    "        end_position = {pid: (date - min_date).dt.days.div(step).astype(int)\n",
    "                        for pid, date in end_date.items()}\n",
    "\n",
    "    # takes dataframe for an individual and makes a string with the events\n",
    "    def make_string(events):\n",
    "        # get pid of individual (required to find correct start and end point)\n",
    "        person = events[pid].iloc[0]\n",
    "\n",
    "        # make a list of maximal length with no events\n",
    "        event_list = [no_event] * (max_length_steps + 1)\n",
    "\n",
    "        # loop over all events the individual has and put code in correct pos.\n",
    "        for pos in events['position'].values:\n",
    "            event_list[pos] = code\n",
    "\n",
    "        event_string = \"\".join(event_list)\n",
    "\n",
    "        # slice to correct start and end of string (if specified)\n",
    "        if first_date:\n",
    "            event_string = event_string[start_position[person]:]\n",
    "        if last_date:\n",
    "            event_string = event_string[:-(max_length_steps - end_position[person])]\n",
    "        return event_string\n",
    "\n",
    "    # new dataframe to store each string for each individual for each code\n",
    "    string_df = pd.DataFrame(index=code_series[pid].unique())\n",
    "\n",
    "    # loop over each code, create aggregate string for each individual, store in df\n",
    "    for code in codes:\n",
    "        code_df = code_series[code_series[col].isin([code])]\n",
    "        stringified = code_df.groupby(pid, sort=False).apply(make_string)\n",
    "        string_df[code] = stringified\n",
    "\n",
    "    if merge:\n",
    "        string_df = interleave_strings(string_df, no_event=no_event, time_sep=time_sep)\n",
    "    return string_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-oIboIXyiGE"
   },
   "source": [
    "# formatting an expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40vOED0gBIzs"
   },
   "source": [
    "## insert_external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TJu1M095GQo"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def insert_external(expr):\n",
    "  \"\"\"\n",
    "  Replaces variables prefixed with @ in the expression with the\n",
    "  value of the variable from the global namespace\n",
    "\n",
    "  Example:\n",
    "      x=['4AB02', '4AB04', '4AB06']\n",
    "      expr = '@x before 4AB02'\n",
    "      insert_external(expr)\n",
    "  \"\"\"\n",
    "  externals = [word.strip('@') for word in expr.split() if word.startswith('@')]\n",
    "  for external in externals:\n",
    "      tmp = globals()[external]\n",
    "      expr = expr.replace(f'@{external} ', f'{tmp} ')\n",
    "  return expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5KRuGdK7RQv"
   },
   "outputs": [],
   "source": [
    "  x_1=['4AB02', '4AB04', '4AB06']\n",
    "  expr = '@x_1 before 4AB02'\n",
    "  insert_external(expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3kN7yEOqjm3V"
   },
   "source": [
    "# get inpatient data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fUwhTeAnzD6q"
   },
   "source": [
    "To test the functions and to calculate the Charslon index we need some data. Here we will use data on hospital visits from Medicare: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3N_2p7tLShTi"
   },
   "outputs": [],
   "source": [
    "# Use pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_jPK3K_0Vzu"
   },
   "outputs": [],
   "source": [
    "# Read synthetic medicare sample data on inpatient hospital stays\n",
    "path = 'https://www.cms.gov/Research-Statistics-Data-and-Systems/Downloadable-Public-Use-Files/SynPUFs/Downloads/'\n",
    "inpatient_file = 'DE1_0_2008_to_2010_Inpatient_Claims_Sample_1.zip'\n",
    "\n",
    "inpatient = pd.read_csv(path+inpatient_file)\n",
    "\n",
    "inpatient.columns = inpatient.columns.str.lower()\n",
    "# easier to use a column called 'pid' than 'desynpuf_id'\n",
    "inpatient['pid']=inpatient['desynpuf_id']\n",
    "\n",
    "#set index to the personal id, but also keep id as a column\n",
    "inpatient = inpatient.set_index('pid', drop=False)\n",
    "inpatient.index.name='pid_index'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GY1LiGNUShT8"
   },
   "outputs": [],
   "source": [
    "# Have a look\n",
    "inpatient.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxQSMPBZMYXe"
   },
   "outputs": [],
   "source": [
    "# make a list of columns with information about diagnostic codes\n",
    "icd_cols = list(inpatient.columns[inpatient.columns.str.startswith('icd9_dgns_cd')])\n",
    "icd_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c23WbohBbO4R"
   },
   "source": [
    "Make a list of all unique ICD9 codes that exist, a all_codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2ObFcg_tlMV"
   },
   "outputs": [],
   "source": [
    "# Codes to calculate CCI using ICD-9 (CM, US, Enhanced)\n",
    "# Source: http://mchp-appserv.cpe.umanitoba.ca/concept/Charlson%20Comorbidities%20-%20Coding%20Algorithms%20for%20ICD-9-CM%20and%20ICD-10.pdf\n",
    "\n",
    "infarction = '''\n",
    "      410* \n",
    "      412*\n",
    "      '''\n",
    "\n",
    "heart_failure = '''\n",
    "        390.91 \n",
    "        402.21 402.11 402.91 \n",
    "        404.01 404.03 404.11 404.13 404.91 404.93 \n",
    "        425.4-425.9 \n",
    "        428*\n",
    "        '''\n",
    "\n",
    "peripheral_vascular = '''\n",
    "        093.0\n",
    "        437.3\n",
    "        440*\n",
    "        441*\n",
    "        443.1-443.9\n",
    "        447.1\n",
    "        557.1 557.9\n",
    "        V43.4\n",
    "        '''\n",
    "\n",
    "cerebrovascular = '''\n",
    "        362.34\n",
    "        430*-438*\n",
    "        '''\n",
    "dementia = '''\n",
    "        290*\n",
    "        294.1\n",
    "        331.2\n",
    "        '''\n",
    "\n",
    "pulmonary ='''\n",
    "      416.8 416.9\n",
    "      490*-505* \n",
    "      506.4\n",
    "      508.1 508.8\n",
    "      '''\n",
    "rheumatic = '''\n",
    "      446.5\n",
    "      710.0-710.4\n",
    "      714.0-714.2 714.8\n",
    "      725*\n",
    "      '''\n",
    "\n",
    "peptic_ulcer = '531*-534*'\n",
    "\n",
    "liver_mild ='''\n",
    "      070.22\n",
    "      070.23\n",
    "      070.32\n",
    "      070.33\n",
    "      070.44\n",
    "      070.54\n",
    "      070.6\n",
    "      070.9\n",
    "      570.*\n",
    "      571.*\n",
    "      573.3 573.4 573.8 573.9\n",
    "      V42.7\n",
    "      '''\n",
    "# Interesting, diabetes seems to be 5 digits long in the data, but not the specified codes\n",
    "diabetes_without_complication = '250.0*-250.3* 250.8* 250.9*'\n",
    "\n",
    "diabetes_with_complication = '250.4*-250.7*'\n",
    "\n",
    "plegia = '''\n",
    "    334.1\n",
    "    342.*\n",
    "    343.*\n",
    "    344.0-344.6\n",
    "    344.9\n",
    "    '''\n",
    "\n",
    "renal = '''\n",
    "    403.01 403.11,403.91 \n",
    "    404.02 404.03 404.12 404.13 404.92 404.93\n",
    "    582.*  \n",
    "    583.0-583.7\n",
    "    585*\n",
    "    586*\n",
    "    588.0\n",
    "    V42.0\n",
    "    V45.1\n",
    "    V56*\n",
    "    '''\n",
    "\n",
    "malignancy = '''\n",
    "    140*-172*\n",
    "    174.0-195.8\n",
    "    200*-208*\n",
    "    238.6\n",
    "    '''\n",
    "\n",
    "liver_not_mild = '''\n",
    "    456.0-456.2\n",
    "    572.2-572.8\n",
    "    '''\n",
    "\n",
    "tumor = '196*-199*'\n",
    "\n",
    "hiv = '042*-044*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lbo8b7wixGH7"
   },
   "source": [
    "Put all the strings that describe the codes for the comorbitities in a single datastructure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-kLY3dNzfRZ"
   },
   "outputs": [],
   "source": [
    "icd9 = unique(df=inpatient, cols = icd_cols, all_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mslB-ZEuxjnm"
   },
   "outputs": [],
   "source": [
    "# A dictionary with names of cormobitities and the associated medical codes\n",
    "\n",
    "code_string = { 'infarction' : infarction, \n",
    "               'heart_failure' : heart_failure, \n",
    "               'peripheral_vascular' : peripheral_vascular, \n",
    "               'cerebrovascular' : cerebrovascular, \n",
    "               'dementia' : dementia, \n",
    "               'pulmonary' : pulmonary, \n",
    "               'rheumatic' : rheumatic, \n",
    "               'peptic_ulcer' : peptic_ulcer, \n",
    "               'liver_mild' : liver_mild, \n",
    "               'diabetes_without_complication' : diabetes_without_complication, \n",
    "               'diabetes_with_complication' : diabetes_with_complication, \n",
    "               'plegia' : plegia, \n",
    "               'renal' : renal, \n",
    "               'malignancy' : malignancy, \n",
    "               'liver_not_mild' : liver_not_mild, \n",
    "               'tumor' : tumor, \n",
    "               'hiv' : hiv}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctiAl4mHbve-"
   },
   "source": [
    "Having created a all_codes, we can use the functions we have created to expand the description for all the different comorbidities to include all the specific codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_YMP5Ukbwrv"
   },
   "outputs": [],
   "source": [
    "codes = {disease : expand_code(codes.split(), \n",
    "                               all_codes=icd9,\n",
    "                               drop_dot=True,\n",
    "                               drop_leading_zero=True) \n",
    "        for disease, codes in code_string.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqLowQI6z9U-"
   },
   "source": [
    "And we can check if it really expanded the codes, for instance by examining the codes for mild liver disease:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POf4RSz0u20L"
   },
   "outputs": [],
   "source": [
    "codes['liver_mild']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXrRqq6xrkHP"
   },
   "source": [
    "In order to do the calculations, we need the weights associated with each comorbidity. These weights are related to the predictive power of the comorbididy for the probability of dying in a given time period. There are a few different standards, but with relatively minor varitions. Here we use the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EKmvFTdx8qT"
   },
   "outputs": [],
   "source": [
    "charlson_points = { 'infarction': 1, \n",
    "                   'heart_failure': 1, \n",
    "                   'peripheral_vascular': 1, \n",
    "                   'cerebrovascular': 1, \n",
    "                   'dementia': 1, \n",
    "                   'pulmonary': 1, \n",
    "                   'rheumatic': 1, \n",
    "                   'peptic_ulcer': 1, \n",
    "                   'liver_mild': 1, \n",
    "                   'diabetes_without_complication': 1, \n",
    "                   'diabetes_with_complication': 2, \n",
    "                   'plegia': 2, \n",
    "                   'renal': 2, \n",
    "                   'malignancy': 2, \n",
    "                   'liver_not_mild': 3, \n",
    "                   'tumor': 6, \n",
    "                   'hiv': 6}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wghaUi_Fj2SS"
   },
   "source": [
    "We also need the function that takes a set of codes and identifies the rows and persons who have the codes (a function we developed in a previous notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted query_language.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.sync import script2notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script2not"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "query language v1",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
